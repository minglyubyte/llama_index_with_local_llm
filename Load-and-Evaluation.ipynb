{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60b5215b-b77d-4d8f-84aa-c95fa8d46660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leo/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "base_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "new_model = \"Llama-2-7b-chat-hf-text2sql\"\n",
    "token = os.getenv('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f43abc26-13dc-4699-b864-8c21d1cdb008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, token = token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model, \n",
    "                                            #quantization_config=quant_config,\n",
    "                                             token = token)\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1e0d87f-2ccb-41c7-968a-77be96ad1b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45c78c89-5bfa-448d-afcb-f7cbf9332950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "Below are sql tables schemas paired with instruction that describes a task. \n",
      "                Using valid SQLite, write a response that appropriately completes the request \n",
      "                for the provided tables. <</SYS>>\n",
      "\n",
      "What was round 7's lowest overall?CREATE TABLE table_name_72 (\n",
      "    overall INTEGER,\n",
      "    round VARCHAR\n",
      ")[/INST]  To complete the task of finding the lowest overall score from round 7, we need to query the `table_name_72` table. The `overall` column in this table represents the score for each round, and the `round` column indicates which round it belongs to.\n",
      "\n",
      "Here's the query to retrieve the lowest overall score from round 7:\n",
      "```sql\n",
      "SELECT MIN(overall) FROM table_name_72 WHERE round = '7';\n",
      "```\n",
      "This query will return the minimum value in the `overall` column for all rows where the `round` column is equal to '7'.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST]<<SYS>>\\nBelow are sql tables schemas paired with instruction that describes a task. \\n                Using valid SQLite, write a response that appropriately completes the request \\n                for the provided tables. <</SYS>>\\n\\nWhat was round 7's lowest overall?CREATE TABLE table_name_72 (\\n    overall INTEGER,\\n    round VARCHAR\\n)[/INST]\"\n",
    "result = pipe(f\"{prompt}\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d29ebb35-bb4b-4175-99cc-11b0ba00452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "del pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d30dae3f-976a-4344-aeac-666c125ef5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.52s/it]\n",
      "/Users/leo/.pyenv/versions/3.10.13/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "peft_params = LoraConfig(\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "peft_model_id = new_model\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, \n",
    "                                             return_dict=True, \n",
    "                                             #load_in_8bit=True, \n",
    "                                             #device_map='auto'\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "peft_model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "011c78db-07b6-4e8d-a538-bb700626ca44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=peft_model, tokenizer=tokenizer, max_length=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "965d5b8b-ff42-41bb-9b0d-3e63709ec796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "Below are sql tables schemas paired with instruction that describes a task. \n",
      "                Using valid SQLite, write a response that appropriately completes the request \n",
      "                for the provided tables. <</SYS>>\n",
      "\n",
      "What was round 7's lowest overall?CREATE TABLE table_name_72 (\n",
      "    overall INTEGER,\n",
      "    round VARCHAR\n",
      ")[/INST]  To complete the task, we need to write a SQL query that retrieves the lowest overall score from round 7. Based on the provided schema, the query should be:\n",
      "```sql\n",
      "SELECT overall FROM table_name_72 WHERE round = '7';\n",
      "```\n",
      "This will retrieve the lowest overall score from round 7, which is stored in the `overall` column of the `table_name_72` table.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST]<<SYS>>\\nBelow are sql tables schemas paired with instruction that describes a task. \\n                Using valid SQLite, write a response that appropriately completes the request \\n                for the provided tables. <</SYS>>\\n\\nWhat was round 7's lowest overall?CREATE TABLE table_name_72 (\\n    overall INTEGER,\\n    round VARCHAR\\n)[/INST]\"\n",
    "result = pipe(f\"{prompt}\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be721e3-4e27-448a-8cc5-6752c0d87552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
